{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe221753-1e22-4911-b409-5a06426b6422",
   "metadata": {},
   "source": [
    "# Processamento de linguagem natural, aula 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b74656-ebad-4540-8de3-ffa367a1c071",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66ecb3d-1304-40a5-a095-7fad5ca33adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (23.2.1)\n",
      "Requirement already satisfied: spacy in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (3.6.1)\n",
      "Requirement already satisfied: goose3 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (3.1.17)\n",
      "Requirement already satisfied: nltk in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (66.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: Pillow in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from goose3) (10.0.0)\n",
      "Requirement already satisfied: lxml in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from goose3) (4.9.3)\n",
      "Requirement already satisfied: cssselect in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from goose3) (1.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from goose3) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from goose3) (2.8.2)\n",
      "Requirement already satisfied: langdetect in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from goose3) (1.0.9)\n",
      "Requirement already satisfied: pyahocorasick in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from goose3) (2.0.0)\n",
      "Requirement already satisfied: click in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from beautifulsoup4->goose3) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: six in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from langdetect->goose3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install spacy goose3 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e301b544-0f04-4056-9bd6-5fa22ccb5beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gustavo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from goose3 import Goose\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "g = Goose()\n",
    "article = g.extract('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "\n",
    "original_sentences = [sentence for sentence in nltk.sent_tokenize(article.cleaned_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ff4db-5d39-4dc4-9a00-3d69595ba787",
   "metadata": {},
   "source": [
    "## Conteúdo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc4ffd-f6c3-4e92-a6a4-bd9f9e8d5f2a",
   "metadata": {},
   "source": [
    "## Resumo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b839e4b-44a5-428d-88db-e5ded320ebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from sumy) (2.31.0)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from sumy) (3.8.1)\n",
      "Collecting chardet (from breadability>=0.1.20->sumy)\n",
      "  Obtaining dependency information for chardet from https://files.pythonhosted.org/packages/38/6f/f5fbc992a329ee4e0f288c1fe0e2ad9485ed064cac731ed2fe47dcc38cbf/chardet-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: lxml>=2.0 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from breadability>=0.1.20->sumy) (4.9.3)\n",
      "Requirement already satisfied: click in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from nltk>=3.0.2->sumy) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from nltk>=3.0.2->sumy) (4.66.1)\n",
      "Requirement already satisfied: setuptools in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from pycountry>=18.2.23->sumy) (66.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (2023.7.22)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=12f50f0c4d71c6325a91e2aeceba37cb0826ccfe4c5b768eede2bf2376636650\n",
      "  Stored in directory: /home/gustavo/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
      "  Building wheel for docopt (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=02b6807592046cc4b595fab635e7ccfaab00a61f32a39262a71c8eb5f4efb3e3\n",
      "  Stored in directory: /home/gustavo/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681833 sha256=159389cc3c2ea98835e0eff8e077a8ee237cfa4c78160d381c37eed60637679d\n",
      "  Stored in directory: /home/gustavo/.cache/pip/wheels/cd/29/8b/617685ed7942656b36efb06ff9247dbe832e3f4f7724fffc09\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, chardet, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feef4cd4-6de2-455f-ad1f-70f572c4409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Sumary - Natural language processing - Wikipedia</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. \n",
      "\n",
      "During this time, the first chatterbots were written (e.g., PARRY). \n",
      "\n",
      "• 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. \n",
      "\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. \n",
      "\n",
      "Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. \n",
      "\n",
      "Since 2015,[21] the statistical approach was replaced by neural networks approach, using word embeddings to capture semantic properties of words. \n",
      "\n",
      "Neural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation. \n",
      "\n",
      "Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. \n",
      "\n",
      "A coarse division is given below. \n",
      "\n",
      "[45] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "\n",
    "# Instanciando processador de texto\n",
    "parser = PlaintextParser.from_string(article.cleaned_text, Tokenizer('english'))\n",
    "\n",
    "# Instanciando o objeto responsável por realizar o resumo do texto\n",
    "summarizer = SumBasicSummarizer()\n",
    "\n",
    "# Cria o resumo do texto com o tamanho desejado\n",
    "sumary = summarizer(parser.document, 10)\n",
    "\n",
    "# Exibindo em HTML\n",
    "display(HTML(f'<h1>Sumary - {article.title}</h1>'))\n",
    "\n",
    "for sentence in sumary:\n",
    "  print(sentence, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f24c5-49d8-4ded-b4da-e8bfba14f436",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pesquisa por palavras chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5b207a5-50e3-4d3f-9782-d9174cec9f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8661325627334373315, 11, 12)]\n",
      "computer\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "\n",
    "text = str(original_sentences[0])\n",
    "\n",
    "# Termos que serão pesquisados\n",
    "search_terms = ['computer']\n",
    "\n",
    "# Tokens dos termos pesquisados\n",
    "search_tokens = [nlp(item) for item in search_terms]\n",
    "\n",
    "# Instancia matcher com base no vocabulário presente no NLP\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Adiciona tokens a serem pesquisados\n",
    "matcher.add('SEARCH', None, *search_tokens)\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# (<id da palavra>, <inicio>, <fim>)\n",
    "print(matches)\n",
    "print(doc[matches[0][1]:matches[0][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee50e75d-5cc5-42c3-8510-eb95f4acb7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>[search_strings_html.upper()]</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Number of Matches: </strong>5</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<blockquote> is an interdisciplinary subfield of <mark>computer</mark> science and linguistics. It<br /><br />. The goal is a <mark>computer</mark> capable of \"understanding\"<br /><br />matching answers), the <mark>computer</mark> emulates natural language understanding (<br /><br />that would fit in a <mark>computer</mark> memory at the time.[3]<br /><br />real-world information into <mark>computer</mark>-understandable data. Examples<br /><br /></blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus.reader import documents\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "\n",
    "search_terms_html = ' '.join(search_terms)\n",
    "marked_text = ' '\n",
    "\n",
    "display(HTML(f'<h1>[search_strings_html.upper()]</h1>'))\n",
    "\n",
    "document = nlp(article.cleaned_text)\n",
    "matches = matcher(document)\n",
    "\n",
    "display(HTML(f'<p><strong>Number of Matches: </strong>{len(matches)}</p>'))\n",
    "\n",
    "number_of_words = 5\n",
    "\n",
    "for i in matches:\n",
    "  start = i[1] - number_of_words\n",
    "    \n",
    "  if start < 0:\n",
    "    start = 0\n",
    "      \n",
    "  for j in range(len(search_tokens)):\n",
    "    if document[i[1]:i[2]].similarity(search_tokens[j])==1.0:\n",
    "      search_text = str(search_tokens[j])\n",
    "      marked_text += str(document[start:i[2] + number_of_words]).replace(search_text, f'<mark>{search_text}</mark>')\n",
    "      marked_text += '<br /><br />'\n",
    "\n",
    "display(HTML(f'<blockquote>{marked_text}</blockquote>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f1ec5-0adb-4ca7-9629-d732ecc89899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
