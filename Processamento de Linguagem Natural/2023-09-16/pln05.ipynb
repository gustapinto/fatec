{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fcbfc6-1db7-46ad-ad20-f8ae0b0e5713",
   "metadata": {},
   "source": [
    "# Processamento de linguagem natural, aula 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d32fb-7e12-4874-b6a3-d5812f71ba61",
   "metadata": {},
   "source": [
    "## Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d57b8117-42bb-45bd-b8fe-260bb285df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gustavo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from goose3 import Goose\n",
    "\n",
    "\n",
    "# Extrai texto que será utilziado depois\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "article = Goose().extract('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "\n",
    "original_sentences = [sentence for sentence in nltk.sent_tokenize(article.cleaned_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2feebe2-91e0-4c03-b402-0387ae8b1cac",
   "metadata": {},
   "source": [
    "## Conteúdo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee6a55-01db-4bd3-ab7c-91813c31bb37",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "É uma forma de representar palavras e tokens de forma numérica, para que algoritimos complexos possam de liddar de forma eficiente com a análise das frases que compõe o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7822af4-2fa7-44f9-92aa-4dead954a7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/1f/ad/9799aabeabcb9a293c87b6f96cc78655b8abc7d35560cd99007093b5d445/scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from scikit-learn) (1.25.2)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/0e/a0/8606a7eef659f3d5f79d9efb92eed3ed1243178f4ae962614e1b202935a6/scipy-1.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scipy-1.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /home/gustavo/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading scipy-1.11.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.0 scipy-1.11.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc12ccb5-e473-439c-b1e9-d92087be6a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'this' 'thrid']\n",
      "\n",
      "[[0 1 1 1 0 0 1 1 0]\n",
      " [0 2 0 1 0 1 1 1 0]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    'This is the first document',\n",
    "    'This document is the second document',\n",
    "    'And this is the thrid one',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Inicializa o vetorizador que analizará as sentenças e as classificará\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# <vetorizador>.fit_transform -> tranforma a lista de sentenças em\n",
    "# uma lista de elementos únicos (features)\n",
    "transform = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# <vetorizador>.get_feature_names_out -> obtém as unique words, contando\n",
    "# as ocorrencias do texto\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(features)\n",
    "print()\n",
    "\n",
    "# obtém o vetor normalizado da Bag of Words das palavras transformadas\n",
    "array = transform.toarray()\n",
    "\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef5d5605-e2ec-45c9-b4aa-61b1f6f5b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gustavo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "transform_sentences = vectorizer.fit_transform(original_sentences)\n",
    "feature_sentences = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(transform_sentences.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab70f0-e7a1-4589-b981-a4acec0fbe07",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency\n",
    "\n",
    "Enquanto usamos Bag Of Words as palavras frequentes dominan a representação, com essas nem sempre sendo a melhor representação do contexto do texto (documento).\n",
    "\n",
    "A técnica de TF-IDF tenta resolver esse problema ao usar o contexto inteiro do documento para analisar o impacto de uma palavra ou sentença no mesmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a97e13a0-743f-429f-9e3f-61d5b847e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 3.60268969\n",
      " 4.70130197 3.44853901 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 4.00815479 4.70130197 4.70130197 3.78501124 3.60268969\n",
      " 4.29583687 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 4.70130197 4.00815479\n",
      " 3.78501124 4.29583687 3.31500761 4.70130197 4.70130197 1.68087709\n",
      " 4.29583687 4.70130197 4.70130197 4.29583687 4.70130197 4.70130197\n",
      " 4.70130197 4.29583687 4.29583687 4.29583687 4.70130197 3.31500761\n",
      " 3.44853901 4.70130197 3.31500761 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 2.56123581 4.00815479 4.70130197\n",
      " 3.60268969 4.00815479 3.78501124 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 2.62186043 4.70130197 3.44853901 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 3.31500761 4.00815479 4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 3.78501124 4.70130197 3.60268969 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 2.68639895 4.70130197 4.70130197\n",
      " 3.19722458 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.29583687 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 2.99655388 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.00815479 4.29583687 4.70130197\n",
      " 4.29583687 4.70130197 3.78501124 3.60268969 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.00815479 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197\n",
      " 4.00815479 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 3.19722458 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.29583687 4.00815479 4.00815479\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.00815479 3.78501124 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197\n",
      " 4.00815479 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.00815479 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.00815479 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.29583687 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 4.00815479 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 4.29583687 2.56123581\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 3.44853901 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.00815479 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 3.78501124 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.29583687 3.78501124 4.70130197\n",
      " 3.44853901 3.19722458 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 3.78501124 4.70130197\n",
      " 4.70130197 4.70130197 3.60268969 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.29583687 4.29583687 4.70130197\n",
      " 4.29583687 4.29583687 4.29583687 4.70130197 1.78353124 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197 4.00815479\n",
      " 4.00815479 4.70130197 4.70130197 3.60268969 4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 3.78501124 4.70130197\n",
      " 4.00815479 4.70130197 4.29583687 4.70130197 4.70130197 3.60268969\n",
      " 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 2.34992672 3.60268969 4.70130197 4.29583687 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 3.78501124 4.70130197 2.17557333\n",
      " 4.00815479 4.29583687 4.29583687 4.00815479 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 2.99655388 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197 4.29583687\n",
      " 4.29583687 4.70130197 4.70130197 4.29583687 4.70130197 3.31500761\n",
      " 4.70130197 4.70130197 4.70130197 4.00815479 4.70130197 4.29583687\n",
      " 2.9095425  4.70130197 4.29583687 4.70130197 4.70130197 4.00815479\n",
      " 4.70130197 4.70130197 4.29583687 3.60268969 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 3.09186406 4.29583687 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.00815479 4.70130197 4.29583687\n",
      " 3.60268969 4.70130197 2.99655388 4.70130197 3.44853901 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 2.56123581 4.70130197 4.70130197 4.70130197 4.00815479\n",
      " 4.00815479 3.19722458 4.70130197 4.70130197 4.70130197 2.5040774\n",
      " 4.70130197 4.29583687 3.78501124 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 1.40546511 4.70130197 4.70130197 4.29583687 4.29583687 4.70130197\n",
      " 2.75539183 4.29583687 4.70130197 4.00815479 4.70130197 4.70130197\n",
      " 4.29583687 2.68639895 4.70130197 3.19722458 4.70130197 4.70130197\n",
      " 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.00815479 4.00815479 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.70130197 4.00815479 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 2.9095425  4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 4.29583687 4.29583687 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.00815479\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.29583687 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 3.19722458 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.00815479 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 2.99655388 3.44853901 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.00815479 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 3.78501124 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 3.78501124\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 3.78501124 4.29583687 4.70130197 4.70130197 4.70130197 2.8294998\n",
      " 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.29583687\n",
      " 4.70130197 2.99655388 4.70130197 4.00815479 4.70130197 4.70130197\n",
      " 3.44853901 4.29583687 4.70130197 4.29583687 2.99655388 4.29583687\n",
      " 4.70130197 4.70130197 4.29583687 3.09186406 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 4.70130197 4.29583687\n",
      " 4.00815479 4.70130197 4.70130197 4.29583687 2.45001018 1.26731477\n",
      " 4.70130197 4.70130197 3.78501124 4.70130197 4.70130197 4.29583687\n",
      " 4.29583687 4.00815479 3.60268969 4.70130197 3.60268969 4.70130197\n",
      " 4.00815479 4.29583687 4.29583687 4.29583687 4.70130197 4.29583687\n",
      " 3.44853901 4.70130197 1.73088751 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.29583687 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 3.19722458 4.70130197 4.29583687\n",
      " 4.00815479 4.70130197 4.70130197 4.29583687 4.70130197 4.29583687\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 3.60268969 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 4.00815479 4.70130197 4.70130197 4.70130197 3.44853901\n",
      " 3.60268969 4.70130197 4.29583687 4.70130197 4.70130197 2.75539183\n",
      " 4.70130197 4.29583687 4.70130197 4.29583687 4.70130197 3.09186406\n",
      " 4.70130197 3.44853901 4.70130197 4.70130197 3.09186406 4.70130197\n",
      " 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197 4.70130197\n",
      " 4.70130197 2.45001018 4.00815479 4.70130197 3.60268969 4.70130197\n",
      " 3.60268969 4.29583687 4.70130197 4.00815479 4.70130197 4.00815479\n",
      " 4.70130197 4.70130197 4.00815479 4.70130197 4.29583687 4.70130197\n",
      " 4.70130197 4.70130197] \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m idf \u001b[38;5;241m=\u001b[39m tf_idf_vectorizer\u001b[38;5;241m.\u001b[39midf_\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(idf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_sentences\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf_idf_x\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "File \u001b[0;32m~/Repositorios/fatec/Processamento de Linguagem Natural/venv/lib/python3.11/site-packages/scipy/sparse/_base.py:340\u001b[0m, in \u001b[0;36m_spbase.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Instancia o vetorizador específico para casos de TF-IDF\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "x_sentences = tf_idf_vectorizer.fit_transform(original_sentences)\n",
    "tf_idf_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# <vetorizador>.idl_ -> Obtém a classificação de idf das sentenças\n",
    "idf = tf_idf_vectorizer.idf_\n",
    "\n",
    "print(idf, '\\n')\n",
    "print(len(x_sentences))\n",
    "print(tf_idf_x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd9ae4-1781-4d8e-a292-46cbd47855f3",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4ef2cd2-e2e2-4b9f-9909-c976107df64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 848)\n",
      "(3, 848)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "concatenate() got multiple values for argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m x_test_similarity_array \u001b[38;5;241m=\u001b[39m x_test_similarity\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_test_similarity_array\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m x_test_similarity_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_similarity_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mx_test_similarity_array\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# calculando a similaridade entre a primeira e a segunda linha\u001b[39;00m\n\u001b[1;32m     20\u001b[0m cosine_similarity(x_test_similarity[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     21\u001b[0m                   x_test_similarity[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: concatenate() got multiple values for argument 'axis'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "print(x_sentences.shape)\n",
    "\n",
    "# Obtém as três primeiras linhas das sentenças\n",
    "x_test_similarity = x_sentences[:3]\n",
    "\n",
    "# Converte as primeiras linhas obtidas para um array\n",
    "x_test_similarity_array = x_test_similarity.toarray()\n",
    "\n",
    "print(x_test_similarity_array.shape)\n",
    "\n",
    "x_test_similarity_array = np.concatenate(x_test_similarity_array,\n",
    "                                         x_test_similarity_array[0].reshape(1, -1),\n",
    "                                        axis=0)\n",
    "\n",
    "# calculando a similaridade entre a primeira e a segunda linha\n",
    "cosine_similarity(x_test_similarity[0].reshape(1, -1),\n",
    "                  x_test_similarity[1].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ce8c8-5e86-45f1-b006-2dbbcbd1a801",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "Desenvolva um algoritmo de busca que, dado um conjunto de conhecimento sobre a cidade de araras, receba do usuário uma pergunta e avalie qual das sentenças da base de conhecimento apresenta maior similaridade e, portanto, poderia ser utilizada como resposta para a pergunta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
